# 毕业设计准备文档
## 1.工作任务书
    2段话（任务内容，怎么做），400字左右，近期完成，上传毕设网站。
## 2.工作计划
    6个阶段，主要是开题，中期，验收，答辩（以学校要求为准）
##3.收集参考文献
    近三年，文献50篇左右（中英2:3），中文15篇+，英文30篇+，综述性文章可以是5~8年。
    中文期刊：ccf  中科院等
    英文期刊：ACL , AAAI , IGCAI , NIPS , CVPR , LCML , ICLI , SCI , TRANS, TIP,
    TPAMI , CL , AI
##4.翻译一篇英文文献，下周老师分发翻译文章
##5.论文格式要求
    50页左右，分为6章
### 第一章 
    引言/绪论5页左右，国内外研究现状（机构组织思路）背景意义，提出研究问题。
### 第二章 相关研究 
    10页左右，与本文课题紧密相关的其他研究，与本文课题紧密相关的其他理论技术。
### 第三章 本文所提的模型
    创新点，15页左右，模型，几个部分，采用总分方式，优化。
## 第四章 实验及结果分析
    15页左右，实验，讨论分析，实验方案
    1.总体方案
    2.目标
    3.方法对比
    4.实验数据：数量，分类
    5.模型配置：参数等
    6.实验结果：各类图表，图表下应有200字左右的解释说明
    7.讨论分析：超参，变量等的影响
### 第五章典型应用（可自拟）
    服务，i/o样例，平台截图
### 第六章总结及其下一步工作



















# 摘要
&nbsp;&nbsp;&nbsp;&nbsp;自顶向下的视觉注意机制已广泛应用于image caption 和 VQA 中。我们提出了一个自底向上和自顶向下相结合的注意力机制，使注意力能够在物体和其他突出的图像区域的水平上进行计算。在我们的方法中，自底向上的机制(基于Faster R-CNN)提取图像区域，每个区域有一个对应的特征向量，而自顶向下的机制确定特征权重。用这个模型我们获得了2017年VQA挑战赛的第一名。
# 1.介绍
视觉注意机制被广泛应用于image caption和VQA。这一机制通过学习聚焦于图像的突出区域来提高性能。
![](http://120.27.245.18:8000/wp-content/uploads/2020/02/9N4MBSW0INUV_01R_E5C-300x231.png)
*图1所示。通常，注意力模型在CNN特征上操作，这些特征对应于大小相等的图像区域(左)的均匀网格。我们的方法可以在物体和其他突出的图像区域的水平上计算注意力(右图)。*

&nbsp;&nbsp;&nbsp;&nbsp;传统的视觉注意力机制都是自顶向下的，这种机制很少考虑如何确定受注意的图像区域。如图1所示，无论图像的内容如何，最终的输入区域都对应于一个大小和形状都相同的神经接受域的统一网格。为了生成更多类似人类的字幕和问题答案，物体和其他突出的图像区域更应该被关注。

&nbsp;&nbsp;&nbsp;&nbsp;本文提出了一种自底向上与自顶向下相结合的视觉注意机制。自底向上机制提用于取图像区域，每个区域由一个集合的卷积特征向量表示。在实践中，我们使用Faster rcnn 实现了自下而上的注意机制。自顶向下机制使用特定任务的上下文来预测图像区域上的注意力分布。然后将参与的特征向量计算为所有区域的图像特征的加权平均值。

&nbsp;&nbsp;&nbsp;&nbsp;我们评估了自底向上和自顶向下相结合对两个任务的结果。我们首先实现了一个image caption模型，该模型在字幕生成过程中多次捕捉图像的显著区域。实证研究发现，自下而上注意力的包含对image captio有显著的积极作用。我们还使用了相同方法，实现了一个VQA模型。使用该模型，我们获得了2017年VQA挑战赛的第一名。代码，模型和预先计算的图像功能可从项目网站(http://www.panderson.me/up-down-attention) 下载。

# 3.方法
&nbsp;&nbsp;&nbsp;&nbsp;给定一个图像I，我们的image caption模型 和 我们的VQA模型都将大小可变的k个图像特征集作为输入，V = {v1，…，vk}，vi∈R，使得每个图像特征对图像的一个显著区域进行编码。空间图像特征V可以定义为我们自底向上注意力模型的输出，也可以按照标准做法定义为CNN的空间输出层。**在3.1节中描述了实现自底向上注意力模型的方法。在第3.2节中，我们概述了图像字幕模型的架构。在第3.3节中，我们概述了VQA模型。**
## 3.1 自底向上的注意模型
&nbsp;&nbsp;&nbsp;&nbsp;空间图像特征V的定义是通用的。然而，在这项工作中，我们根据边界框定义空间区域，并使用Faster R-CNN实现自下而上的注意力。Faster R-CNN是一个对象检测模型，用于识别对象的类别并且用边框定位对象在图片中的位置。其他 region proposal networks 也可以训练成一种注意力机制。

&nbsp;&nbsp;&nbsp;&nbsp;在这项工作中，我们将Faster R-CNN与ResNet-101 结合使用。为了生成用于image caption或 VQA 的图像特征的输出集 V，我们获取模型的最终输出，并使用IoU阈值对每个对象类别执行非最大抑制。然后，我们选择所有类别检测概率超过置信度阈值的所有区域。对于每个选定区域i，vi被定义为该区域的平均池化卷积特征，因此图像特征向量的维D为2048。

&nbsp;&nbsp;&nbsp;&nbsp;为了预训练 bottom-up attention 模型，我们首先使用ResNet-101在ImageNet上预训练用于分类的Faster R-CNN。然后我们使用 Visual Genome 数据进行训练。为了学习良好的特征表示，我们添加了一个额外的训练输出来预测属性类（除了对象类之外）。为了预测区域i的属性，我们将平均池化卷积特征 vi 与学习到的真值（ground-truth ）对象类的嵌入连接起来，并将其输入到附加的输出层中，从而定义每个属性类和“无属性”类的softmax分布。

&nbsp;&nbsp;&nbsp;&nbsp;原始的Faster R-CNN多任务损失函数包含四个组件，分别定义了RPN和对象分类和边界框回归输出。我们保留了这些组件，并添加了一个额外的多类损失组件来训练属性预测器。在图2中，我们提供了一些模型输出的示例。

![](http://120.27.245.18:8000/wp-content/uploads/2020/02/WL368413Q2DIFCDBOEQ3-188x300.png)
*图2。模型输出的示例。每个边界框都用一个属性类和一个对象类来标记。但是请注意，在image caption 和 VQA中，我们只使用特征向量，而不是预测的标签。*

## 3.2 Image Caption Model
&nbsp;&nbsp;&nbsp;&nbsp;给定一组图像特征V，我们的模型使用现有的部分输出序列作为上下文，使用“软”的自上而下的注意力机制在描述生成过程中对每个特征进行加权。这种方法与以前的模型大致相似。但是，下面概述的特定设计构成了一个相对简单而高性能的基线模型。即使没有自下而上的注意机制，我们的字幕模型仍可以在大多数评估指标上实现与最新技术相当的性能（请参阅表1）。

&nbsp;&nbsp;&nbsp;&nbsp;在较高的层次上，字幕模型使用标准的两个LSTM层组成。在以下各节中，我们将使用以下表示法在单个时间步上引用LSTM的操作：

`$$ h_t = LSTM(x_t , h_{t-1}) $$  (1)`
其中$$x_t$$为LSTM输入向量，$$h_t$$为LSTM输出向量。在这里，我们忽略了记忆细胞的传播，以方便标记。现在我们来描述LSTM输入向量$$x_t$$和输出向量$$h_t$$的表达式。整个字幕模型如图3所示。

![](http://120.27.245.18:8000/wp-content/uploads/2020/02/NB4BXS_XGS99J5_0S4ZB-300x214.png)
*图3。概述Caption model。两个LSTM层用于选择性地关注空间图像特征{v1，…,vk}。这些特征可以定义为CNN的空间输出，或者按照我们的方法，使用自下而上的注意力生成。*

### 3.2.1Top-Down Attention LSTM

&nbsp;&nbsp;&nbsp;&nbsp; 在Caption model中，我们将第一个LSTM层描述为自顶向下的视觉注意力模型，而将第二个LSTM层描述为语言模型，并在接下来的等式中使用上标指示每个层。注意，自底向上的注意力模型在3.1节中进行了描述，在这一节中，它的输出被简单地认为是特征V。每个时间步的Attention LSTM的输入向量由LSTM语言模型的前一个输出组成，再加上平均池化的图像特征    `$$\bar v = \frac{1}{k} \sum_{i} v_i $$`，以及之前生成的单词的编码，具体如下:
					`$$ x^1_t = [h^2_{t-1},\bar v,W_e \prod_t] $$`  (2)

其中$$ W_e \in R^{E*|\sum|} $$ 是一个词词汇 $$\sum $$嵌入矩阵,和$$\prod_t$$是一个在t时间的 独热编码输入单词。这些输入分别为LSTM提供有关LSTM语言模型的状态，图像的大概内容以及到目前为止生成的部分描述信息。嵌入单词是从随机初始化中学习而无需预先训练的。

给定Attention LSTM 的输出 $$h^1_t$$,在每个时间步 t 我们 为每个图像特征$$v_i $$ 生成一个规范化的attention 权重 $$\alpha_{i,t} $$ 如下所示：
`$$ a_{i,t} = w^T_a tanh(W_{va}v_i + W_{ha}h^1_t) $$` (3)

`$$\alpha_{t} = softmax(a_t) $$` (4)

其中$$W_{va} \in R^{H*V} ,W_{ha} \in R^{H*M} , w_a \in R^{H} $$ 是学习到的参数。作为LSTM语言模型输入的图像特征被计算为所有输入特征的凸组合：
$$ \bar v = \sum_{i=1}^k \alpha_{i,t} v_i  $$  (5)

### 3.2.2 LSTM 语言模型
LSTM 语言模型的输入包括有图像特征，并与Attention LSTM的输出相连接，如下所示:
$$x^2_t = [\bar v_t,h^1_t]$$	(6)
用符号$$y_{1 : T}$$ 表示一个词序列（$$y_1,....,y_T$$),在每个时间步t上可能的输出字的条件分布表示为：
$$p(y_t | y_{1:t-1}) = softmax (W_p h^2_t + b_p)$$   (7)

其中$$W_p \in  R^{M*|\sum|} $$ ,$$b_p \in  R^{|\sum|} $$ 分别是学习到的权重和偏置。

完整输出序列上的分布计算为条件分布的乘积：
$$p(y_{1:T}) = \prod_{t=1}^T p(y_t|y_{1:t-1})$$		(8)

### 3.2.3 目标
给定一个目标真值序列$$y^*_{1:T}$$和Caption模型参数 $$\theta$$,我们最小化下面的交叉熵损失:

$$L_{XE}(\theta) = - \sum_{t=1}^T  log(p_{\theta}(y_t^* | y^*_{1:t-1})) $$  (9)

为了与最近的工作[33]进行公平的比较，我们还报告了针对苹果酒[42]优化的结果。从交叉熵训练模型初始化，我们寻求最小化负期望分数:
$$L_R(\theta) = -E_{y_{1:T}\sim p_{\theta}}[r(y_{1:T})] $$	(10)

其中r是评分函数（比如CIDEr）。按照自临界序列训练(SCST)方法，可以近似得到这种损失的梯度:
$$ \Delta_{\theta} L_R(\theta) \sim -(r(y_{1:T}^s) - r(\hat y_{1:T})) \Delta_{\theta} log p_{\theta}(y_{1:T}^s) $$
